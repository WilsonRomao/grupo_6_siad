# Projeto de Data Warehouse: An√°lise de Casos de Dengue

Este projeto √© um pipeline de ETL (Extract, Transform, Load) completo, projetado para coletar, processar e carregar dados de diversas fontes em um Data Warehouse (DW). O objetivo √© centralizar informa√ß√µes sobre casos de dengue, clima e fatores socioecon√¥micos para permitir an√°lises e a cria√ß√£o de dashboards.

## üéØ Objetivo do Projeto

O Data Warehouse constru√≠do por este pipeline permite correlacionar dados de sa√∫de p√∫blica (casos, √≥bitos, hospitaliza√ß√µes por dengue) com dados externos, visando responder perguntas como:

  * Qual a correla√ß√£o entre a precipita√ß√£o e a temperatura m√©dia com o aumento de casos de dengue nas semanas seguintes?
  * Existe uma rela√ß√£o entre indicadores de saneamento b√°sico (√°gua e esgoto) e a incid√™ncia de dengue em diferentes capitais?
  * Como os casos de dengue evolu√≠ram ao longo dos anos, segmentados por faixa et√°ria e sexo, nas principais capitais do Brasil?

## Data Model (Esquema Estrela)

O pipeline foi desenhado para popular um esquema estrela (Star Schema) simples, ideal para ferramentas de Business Intelligence (como Power BI ou Tableau).

  * **Dimens√µes (Tabelas de Atributos):**

      * `dim_local`: Cont√©m as 27 capitais do Brasil (UF, Nome, C√≥digo IBGE).
      * `dim_tempo`: Dimens√£o de calend√°rio com granularidade di√°ria, enriquecida com `ano_epidemiologico` e `semana_epidemiologica`.

  * **Fatos (Tabelas de M√©tricas):**

      * `fato_casos_dengue`: M√©tricas de dengue (casos, √≥bitos, hospitaliza√ß√µes) com granularidade **di√°ria**.
      * `fato_clima`: M√©tricas de clima (temperatura m√©dia, precipita√ß√£o total) com granularidade **semanal (epidemiol√≥gica)**.
      * `fato_socioeconomico`: M√©tricas de saneamento (popula√ß√£o, √°gua, esgoto) com granularidade **anual**.

<!-- end list -->

```
                 +-------------------+
                 |    dim_local      |
                 +-------------------+
                 |*id_local (PK)     |
                 | uf                |
                 | nome_municipio    |
                 | cod_municipio     |
                 +-------------------+
                      ^      ^      ^
                      |      |      |
+---------------------+      |      +-----------------------+
|                            |                              |
|   +---------------------+  |  +-------------------------+ |
|   | fato_casos_dengue   |  |  |   fato_socioeconomico   | |
|   +---------------------+  |  +-------------------------+ |
|   |*id_local (FK)       |  |  |*id_local (FK)           | |
|   |*id_tempo (FK)       |  |  |*id_tempo (FK) (anual)   | |
|   | num_casos           |  |  | num_populacao           | |
|   | num_obitos          |  |  | num_agua_tratada        | |
|   | ... (outras m√©tricas) |  |  | num_esgoto              | |
|   +---------------------+  |  +-------------------------+ |
|             ^              |                ^             |
|             |              |                |             |
(di√°rio)      |      +-------+-------+        | (anual)
              +------+   dim_tempo   +--------+
                     +---------------+
(semanal)     |      |*id_tempo (PK) |        |
              |      | data_completa |        |
+-------------+      | ano           |        |
|                    | mes           |        |
| +----------------+ | dia           |        |
| |  fato_clima    | | ano_epid...   |        |
| +----------------+ | semana_epid...|        |
| |*id_local (FK)  | +---------------+        |
| |*id_tempo (FK)  |                          |
| | temp_media     |                          |
| | precip_total   |                          |
| +----------------+                          |
|                                             |
+---------------------------------------------+
```

## ‚öôÔ∏è Arquitetura e Funcionamento do Pipeline

O pipeline √© orquestrado pelo script `run_pipeline.py`, que executa todos os passos na ordem correta de depend√™ncia.

1.  **`create_tables.py`**

      * **O que faz:** L√™ o arquivo `dw_schema/create_dw.sql` e as credenciais de `configs/db_config.yml`.
      * **Resultado:** Cria toda a estrutura de tabelas (DIMs e FATOs) no banco de dados MySQL.

2.  **`cria_dimensoes.py`**

      * **O que faz:** Gera as dimens√µes que s√£o pr√©-requisito para os demais scripts.
      * **`dim_local`:** L√™ o arquivo `.../RELATORIO_DTB_BRASIL_2024_MUNICIPIOS.xls`, filtra apenas as 27 capitais e resolve ambiguidades (ex: "Palmas", "Campo Grande").
      * **`dim_tempo`:** Cria um calend√°rio completo, dia a dia, para o intervalo de anos definido (ex: 2017-2022) e calcula a semana epidemiol√≥gica.
      * **Resultado:** Salva `dim_local.csv` e `dim_tempo.csv` na pasta `dados/processados/`.

3.  **`etl_dengue.py`**

      * **O que faz:** Processa os dados brutos de dengue (arquivos `DENGBR*.csv`).
      * **Otimiza√ß√£o:** Carrega `dim_local` primeiro e filtra os dados brutos *durante a leitura*, economizando mem√≥ria.
      * **Transforma√ß√£o:** Limpa dados, calcula idade, trata nulos, e mapeia as chaves `id_local` e `id_tempo` (di√°ria).
      * **Resultado:** Agrega os dados por dia/local e salva `fato_casos_dengue.csv` em `dados/processados/`.

4.  **`etl_clima.py`**

      * **O que faz:** Processa m√∫ltiplos arquivos de clima do INMET (arquivos `INMET_*.CSV`).
      * **Transforma√ß√£o:** L√™ os metadados do cabe√ßalho de cada arquivo para identificar a capital. Trata dados hor√°rios, interpola valores faltantes (NaNs) e agrega para o n√≠vel **di√°rio**.
      * **Agrega√ß√£o:** Mapeia `id_local` e `id_tempo` e, em seguida, agrupa os dados di√°rios no n√≠vel **semanal (epidemiol√≥gico)**, calculando m√©dias (temperatura) e somas (precipita√ß√£o).
      * **Resultado:** Salva `fato_clima.csv` em `dados/processados/`.

5.  **`etl_socioeconomico.py`**

      * **O que faz:** Processa os dados do SNIS (Sistema Nacional de Informa√ß√µes sobre Saneamento).
      * **Transforma√ß√£o:** Filtra os dados anuais para as capitais presentes na `dim_local`.
      * **Agrega√ß√£o:** Mapeia `id_local` e `id_tempo` aplicando a regra de neg√≥cio de granularidade **anual** (buscando o `id_tempo` referente ao dia 1¬∫ de Janeiro de cada ano).
      * **Resultado:** Salva `fato_socioeconomico.csv` em `dados/processados/`.

6.  **`load.py`**

      * **O que faz:** Etapa final de carga no Data Warehouse.
      * **Idempot√™ncia:** Primeiro, executa `DELETE FROM` em todas as tabelas (em ordem reversa, fatos primeiro) para garantir que o pipeline possa ser re-executado sem duplicar dados.
      * **Carga:** L√™ todos os arquivos CSV da pasta `dados/processados/` e os carrega para as tabelas correspondentes no MySQL usando `pandas.to_sql`.
      * **Transa√ß√£o:** Gerencia a conex√£o e garante que todas as cargas sejam "commitadas" juntas no final, ou revertidas (rollback) em caso de erro.

## üìÇ Estrutura de Diret√≥rios Esperada

Para que o pipeline funcione, os arquivos devem estar organizados da seguinte forma:

```
/seu_projeto/
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ db_config.yml           # <-- VOC√ä PRECISA CRIAR
‚îú‚îÄ‚îÄ dados/
‚îÇ   ‚îú‚îÄ‚îÄ brutos/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dengue/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DENGBR20.csv      # (Exemplo de dado bruto)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ meteorologico/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ apenas_capitais/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ INMET_...csv  # (Exemplo de dado bruto)
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ local/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RELATORIO_DTB_BRASIL_2024_MUNICIPIOS.xls
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ br_mdr_snis_municipio_agua_esgoto.csv
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ processados/
‚îÇ       ‚îú‚îÄ‚îÄ (arquivos .csv ser√£o gerados aqui)
‚îÇ
‚îú‚îÄ‚îÄ dw_schema/
‚îÇ   ‚îî‚îÄ‚îÄ create_dw.sql           # <-- VOC√ä PRECISA CRIAR
‚îÇ
‚îú‚îÄ‚îÄ create_tables.py
‚îú‚îÄ‚îÄ cria_dimensoes.py
‚îú‚îÄ‚îÄ etl_clima.py
‚îú‚îÄ‚îÄ etl_dengue.py
‚îú‚îÄ‚îÄ etl_socioeconomico.py
‚îú‚îÄ‚îÄ load.py
‚îú‚îÄ‚îÄ run_pipeline.py
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt            # <-- Crie com o conte√∫do abaixo
```

## üöÄ Como Executar

### 1\. Pr√©-requisitos

  * Python 3.8+
  * Um servidor MySQL (local ou em nuvem, ex: Docker).
  * Os arquivos de dados brutos listados acima, nas pastas corretas.

### 2\. Arquivos de Configura√ß√£o

**`requirements.txt`**
Crie este arquivo na raiz do projeto:

```
pandas
numpy
SQLAlchemy
PyMySQL
PyYAML
openpyxl  # Necess√°rio para ler o .xls do IBGE
```

**`configs/db_config.yml`**
Crie a pasta `configs` e o arquivo `db_config.yml` com suas credenciais:

```yaml
mysql:
  host: "localhost"     # ou o IP do seu container/servidor
  port: 3306
  user: "root"
  password: "sua_senha_segura"
  database: "dw_dengue"   # O banco de dados que ser√° criado
```

**`dw_schema/create_dw.sql`**
Crie a pasta `dw_schema` e o arquivo `create_dw.sql` com o DDL (Data Definition Language) para criar seu banco e suas tabelas.

*Exemplo m√≠nimo:*

```sql
-- Garante que o banco de dados e o schema sejam criados
CREATE DATABASE IF NOT EXISTS dw_dengue CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
USE dw_dengue;

-- (Cole aqui os comandos CREATE TABLE para dim_local, dim_tempo, e todas as fatos)
-- Ex:
CREATE TABLE IF NOT EXISTS dim_local (
    id_local INT PRIMARY KEY,
    uf VARCHAR(2),
    cod_municipio VARCHAR(7),
    nome_municipio VARCHAR(100)
);

-- ... (etc. para todas as outras tabelas) ...
```

### 3\. Instala√ß√£o e Execu√ß√£o

1.  **Instale as depend√™ncias:**

    ```bash
    pip install -r requirements.txt
    ```

2.  **Execute o pipeline mestre:**
    O orquestrador `run_pipeline.py` cuidar√° de executar todos os scripts na ordem correta.

    ```bash
    python run_pipeline.py
    ```

Se tudo estiver configurado corretamente, o script executar√° todos os 6 passos, imprimindo o progresso no console, e ao final seus dados estar√£o carregados no Data Warehouse MySQL.